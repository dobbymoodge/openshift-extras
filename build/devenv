#!/usr/bin/env ruby

$: << File.expand_path(File.dirname(__FILE__))
require 'origin_constants'
#unless ENV['SKIP_SETUP']
#  require 'lib/openshift/setup_helper'
#  SetupHelper::ensure_build_requirements
#end
require File.join('lib', '..', '..', '..', "#{DEV_TOOLS_REPO}", 'build', 'lib', 'openshift')
require File.join('lib', '..', '..', '..', "#{DEV_TOOLS_REPO}", 'build', 'builder')
require 'rubygems'
require 'thor'
require 'fileutils'
require 'lib/openshift'
require 'pp'
require 'yaml'
require 'builder'

include FileUtils

module Origin
  class BuilderPlugin < OpenShift::Builder
    include OpenShift::BuilderHelper

    desc "build_livecd", "Build a livecd"
    def build_livecd
      basedir = "/root"
      remix_dir = "/root/origin-server/remix"
      remix_ks = "openshift-origin-remix.ks"  
      
      FileUtils.mkdir_p remix_dir
      git_rev = `git log --pretty="format:%H %cd" -1`
      
      system "rm -f #{remix_dir}/#{remix_ks}"
      ks_data = File.read("/usr/share/openshift/kickstarts/#{remix_ks}").gsub(/#ADDITIONAL REPOS/, "repo --name=local-build --baseurl=file://#{basedir}/origin-rpms\n#ADDITIONAL REPOS")
      ks_data.gsub!(/#GIT_REV#/,git_rev)
      
      if File.exist?("#{basedir}/extras")
        system "createrepo #{basedir}/extras"
        ks_data.gsub!(/#ADDITIONAL REPOS/, "repo --name=local-extras --baseurl=file://#{basedir}/extras\n#ADDITIONAL REPOS")
        #ks_data.gsub!(/#cartridge/,"cartridge")
      end
      File.open("#{remix_dir}/#{remix_ks}", 'w') do |out|
        out << ks_data
      end
      
      run "/sbin/service mongod stop"
      run "/usr/sbin/setenforce 0"
      run "cd #{remix_dir} && livecd-creator -c openshift-origin-remix.ks -f openshift_origin --cache=cache -d -v --logfile=livecd.log"
      run "/usr/sbin/setenforce 1"
      run "/sbin/service mongod start"
    end

    no_tasks do
      alias_method :old_install_required_packages, :install_required_packages
    end

    def install_required_packages
      base_os = guess_os
      puts "Install packages required for build"
      
      if `rpm -q activemq`.match(/is not installed/)
        run "yum erase -y activemq"
        if base_os == "fedora"
          run "yum install -y https://mirror.openshift.com/pub/origin-server/fedora-18/x86_64/activemq-5.6.0-4.fc18.x86_64.rpm"
          run %{ yum erase -y mcollective mcollective-common mcollective-client;\
                 yum install -y yum update -y https://mirror.openshift.com/pub/origin-server/fedora-17/x86_64/mcollective-2.2.0-2.fc17.noarch.rpm \
                 https://mirror.openshift.com/pub/origin-server/fedora-17/x86_64/mcollective-common-2.2.0-2.fc17.noarch.rpm \
                 https://mirror.openshift.com/pub/origin-server/fedora-17/x86_64/mcollective-client-2.2.0-2.fc17.noarch.rpm;
              }
        elsif(base_os == "rhel" or base_os == "centos")
          run "yum install -y activemq"
        end
      end
      
      if base_os == "fedora"
        run "yum -y update --enablerepo updates-testing ruby ruby-irb ruby-libs ruby-devel"
        run("gem install rspec -v '1.1.12'", options)
      end

      run "wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat/jenkins.repo"
      run "rpm --import http://pkg.jenkins-ci.org/redhat/jenkins-ci.org.key"      
      run "yum install -y tito make tig mlocate bash-completion activemq-client zlib-devel"

      old_install_required_packages
    end
    
    desc "local_build", "Builds and installs all packages locally"
    method_option :base_os, :default => nil, :desc => "Operating system for Origin (fedora or rhel)"
    method_option :verbose, :type => :boolean, :desc => "Enable verbose logging"
    method_option :clean_packages, :type => :boolean, :desc => "Erase existing packages before install?"
    method_option :update_packages, :type => :boolean, :desc => "Run yum update before install?"
    method_option :incremental, :type => :boolean, :desc => "Build only the changed packages"
    def local_build
      options.verbose? ? $log.level = Logger::DEBUG : $log.level = Logger::ERROR
      def_constants(guess_os(options.base_os))
      
      if options.incremental
        options.retry_failure_with_tag = false
        update
      else
        FileUtils.rm_f "/etc/yum.repos.d/local-openshift-origin.repo"
        FileUtils.rm_rf "/tmp/tito"
        
        packages = get_packages(false, true).values

        if options.clean_packages?
          run("yum clean all", options)
          package_names = "\"#{packages.join("\" \"")}\""
          puts "Removing stale packages..."
          run("yum erase -y #{package_names}", options)
        end
        
        if options.update_packages?
          puts "Updating all packages on the system..."
          run("yum update -y", options)
          puts "Done"
        end

        find_and_build_specs

        FileUtils.rm_rf "/data/origin-rpms"
        FileUtils.rm_rf "/data/origin-srpms"
        FileUtils.mkdir_p "/data/origin-rpms"
        FileUtils.mkdir_p "/data/origin-srpms"        
        File.open("/etc/yum.repos.d/local-openshift-origin.repo", 'w') do |out|
          out << %{
[openshift-origin]
name    = openshift-origin
baseurl = file:///data/origin-rpms
gpgcheck= 0
enabled = 1
retries = 0
priority= 1
          }
        end
        run("cp /tmp/tito/x86_64/*.rpm /data/origin-rpms/; cp /tmp/tito/noarch/*.rpm /data/origin-rpms/; createrepo /data/origin-rpms/", options)
        run("cp /tmp/tito/*.src.rpm /data/origin-srpms/; chown -R #{SSH_USER}:#{SSH_USER} /data/origin-srpms", options)
        run("yum clean all")
        
        packages_to_install = packages.select{ |p| not IGNORE_PACKAGES.include?(p.name) }
        package_list = "\"#{packages_to_install.join("\" \"")}\""
        run("yum install -y #{package_list}", options)

        #mark all packages as sync'd
        get_sync_dirs
      end
    end


    no_tasks do
      def ssh_user
        return SSH_USER
      end

      def download_artifacts(hostname)
        puts "Downloading logs and screenshots..."
        `rm -rf rhc/log origin-rpms origin-srpms; mkdir -p rhc/log/; pushd rhc/log > /dev/null; mkdir -p broker ruby193-mcollective system screenshots broker-profiler coverage cucumber; popd > /dev/null`
        scp_from(hostname, "/tmp/rhc/*", "rhc/log", 60, ssh_user)
        scp_from(hostname, "/var/log/openshift/broker/*", "rhc/log/broker", 60, ssh_user)
        scp_from(hostname, "/var/log/openshift/node/*", "rhc/log/node", 60, ssh_user)
        scp_from(hostname, "/var/log/openshift/user_action.log", "rhc/log/broker/user_action.log", 60, ssh_user)
        scp_from(hostname, "/var/log/node-web-proxy/*", "rhc/log/node-web-proxy", 60, ssh_user)
        scp_from(hostname, "/var/log/ruby193-mcollective.*", "rhc/log/ruby193-mcollective", 60, ssh_user)
        scp_from(hostname, "/var/log/httpd/access_log", "rhc/log/system/access_log.log", 60, ssh_user)
        scp_from(hostname, "/var/log/httpd/error_log", "rhc/log/system/error_log.log", 60, ssh_user)
        scp_from(hostname, "/var/log/yum.log", "rhc/log/system/yum.log", 60, ssh_user)
        scp_from(hostname, "/var/log/messages", "rhc/log/system/messages.log", 60, ssh_user)
        scp_from(hostname, "/var/log/dmesg", "rhc/log/system/dmesg.log", 60, ssh_user)
        scp_from(hostname, "/var/log/secure", "rhc/log/system/secure.log", 60, ssh_user)
        scp_from(hostname, "/var/log/audit/audit.log", "rhc/log/system/audit.log", 60, ssh_user)
        scp_from(hostname, "/tmp/rhc/*_coverage", "rhc/log/coverage", 60, ssh_user)
        scp_from(hostname, "/tmp/rhc/screenshots/*", "rhc/log/screenshots", 60, ssh_user)
        scp_from(hostname, "/tmp/rhc/benchmark.csv", "rhc/log", 60, ssh_user)
        scp_from(hostname, "/tmp/broker-profiler/*", "rhc/log/broker-profiler", 60, ssh_user)
        scp_from(hostname, "/tmp/rhc/*_coverage", "rhc/log/coverage", 60, ssh_user)
        scp_from(hostname, "/data/openshift-test/broker/log/*", "rhc/log/origin-broker", 60, ssh_user)
        scp_from(hostname, "/data/openshift-test/broker/test/coverage/*", "rhc/log/origin-broker/test/coverage", 60, ssh_user)
        scp_from(hostname, "/data/openshift-test/node/test/coverage/*", "rhc/log/node/test/coverage", 60, ssh_user)
        scp_from(hostname, "/tmp/rerun_*", "rhc/log/cucumber", 60, ssh_user)

        puts "Done"
      end
      
      def validate_instance(hostname, num_tries=1)
      end
      
      def update_cucumber_tests(hostname, repo_parent_dir="/data", user="root")
        ssh(hostname, "
          rm -f #{repo_parent_dir}/openshift-test/tests; 
          ln -s #{repo_parent_dir}/openshift-test/controller/test/cucumber #{repo_parent_dir}/openshift-test/tests ;", 60, false, 2, user)
      end

      def setup_verifier(hostname, branch)
        update_remote_tests(hostname, branch, "/data", ssh_user)
      end

      def rpm_manifest(hostname, sshuser="root")
        print "Retrieving RPM manifest.."
        manifest = ssh(hostname, 'rpm -qa | grep -E "(rhc|openshift)" | grep -v cartridge', 60, false, 1, sshuser)
        manifest = manifest.split("\n").sort.join(" / ")
        # Trim down the output to 255 characters
        manifest.gsub!(/rubygem-([a-z])/, '\1')
        manifest.gsub!('openshift-origin-', '')
        manifest.gsub!('ruby193-mcollective-', 'mco-')
        manifest.gsub!('.fc16', '')
        manifest.gsub!('.noarch', '')
        manifest.gsub!(/\.git\.[a-z0-9\.]+/, '')
        manifest = manifest[0..254]
        puts "Done"
        return manifest
      end

      def idle_all_gears(hostname)
        puts "Idling all gears on remote instance: #{hostname}"
        ssh(hostname, "sudo bash -c /sbin/service ruby193-mcollective stop; /sbin/service ruby193-mcollective start;", 240, false, 1, options.ssh_user)
        ssh(hostname, 'for dir in /var/lib/openshift/*; do if [ -d $dir ] && [ ! -h $dir ] ; then sudo bash -c "oo-idler -n -u `basename $dir`"; fi; done;', 240, false, 1, options.ssh_user)
        ssh(hostname, 'sudo bash -c "/sbin/service httpd graceful"', 240, false, 1, options.ssh_user)
        puts "Done"
      end

      def fuse_test_impl(tag, hostname, instance, conn, options, image_id=nil)
        begin        
          validate_instance(hostname, 4)
          mcollective_logs(hostname) if options.mcollective_logs?
          idle_all_gears(hostname) unless options.official?
          reset_test_dir(hostname, false, options.ssh_user)

          broker_profiler(hostname) if options.profile_broker?

          # Some of the tests expect quickstarts.json to be non-empty
          qs_path = File.expand_path(File.dirname(__FILE__) + "/../misc/devenv/quickstarts.json")
          scp_to(hostname, qs_path, "~/", 60*10, 5, ssh_user)
          ssh(hostname, "sudo mv ~/quickstarts.json /etc/openshift/", 60 * 2, true, 1, options.ssh_user)

          # Some of the tests require that DEFAULT_MAX_GEARS be set to 3
          ssh(hostname, 'sudo sed -i "s/^DEFAULT_MAX_GEARS=.*/DEFAULT_MAX_GEARS=\"3\"/" /etc/openshift/broker-dev.conf; sudo service openshift-broker restart',
              60 * 2, true, 1, options.ssh_user) 
          
          # test dependencies only
          ssh(hostname, "sudo yum install -y --disableplugin=priorities ruby193-rubygem-mocha", 60 * 5, true, 1, ssh_user)
          ssh(hostname, "sudo yum install -y ruby193-rubygem-net-scp ruby193-ruby-mysql ruby193-rubygem-fakefs", 60 * 5, true, 1, ssh_user)

          # non-test dependencies that should be required by cartridges
          ssh(hostname, "sudo yum install -y python-virtualenv python27-python-virtualenv python33-python-virtualenv mod_wsgi python27-mod_wsgi python33-mod_wsgi jboss-eap6-modules jboss-eap6-index jbossas-product-eap", 60 * 5, true, 1, ssh_user)

          puts "Uploading fuse rpm"
          rpm_path = File.expand_path("/var/opt/fuse/*.rpm")
          scp_to(hostname, rpm_path, "/tmp", 4800, 2, ssh_user)
          output, exit_code = ssh(hostname, "sudo rpm -i /tmp/*rpm", 240, true, 1, ssh_user)        
          print_highlighted_output('Install Fuse RPM Output', output)
          exit 1 unless exit_code == 0
          output, exit_code = ssh(hostname, "sudo oo-admin-ctl-cartridge -c import-node --activate", 240, true, 1, ssh_user)
          print_highlighted_output('Import Fuse Cartridge Output', output)
          exit 1 unless exit_code == 0
          output, exit_code = ssh(hostname, "sudo sed -i s/quota_blocks=1048576/quota_blocks=2048576/ /etc/openshift/resource_limits.conf", 240, true, 1, ssh_user)
          print_highlighted_output('Update node resource limits', output)
          exit 1 unless exit_code == 0
          output, exit_code = ssh(hostname, "sudo service ruby193-mcollective restart", 240, true, 1, ssh_user)
          print_highlighted_output('Restart node', output)
          exit 1 unless exit_code == 0
          
          
          queue=[]
          queue << build_cucumber_command("Fuse Tests", ["@cartridge_fuse"])
          all_pass &= run_tests_with_retry([queue], hostname, 'ec2-user', false)
          exit 1 unless all_pass
          validate_instance(hostname, 4)
          if options.official?
            image_id = image_id ? image_id : instance.image_id
            # Mark the image as verified
            image = conn.images[image_id]
            verify_image(image)

#            puts "Sending QE ready email..."
#            begin
#              send_verified_email(image_id, image.name)
#            rescue Exception => e
#              puts "Failed sending email with message: #{e.message}"
#            end
          elsif !options.terminate?
            idle_all_gears(hostname)
          end

          broker_profiler(hostname, enable=false) if options.profile_broker?


        ensure
          # add frontend config for any gears that may be left over from testing
          ssh(hostname, "sudo bash -c \"/usr/bin/oo-frontend-plugin-modify --rebuild --confirm\"", 120, false, 1, options.ssh_user)

          # reset quickstarts.json back to an empty list
          ssh(hostname, 'sudo bash -c echo "[]" > /etc/openshift/quickstarts.json', 60 * 2, true, 1, options.ssh_user) 

          # reset DEFAULT_MAX_GEARS="100"
          ssh(hostname, 'sudo sed -i "s/^DEFAULT_MAX_GEARS=.*/DEFAULT_MAX_GEARS=\"100\"/" /etc/openshift/broker-dev.conf; sudo service openshift-broker restart', 
              60 * 2, true, 1, options.ssh_user) 

          #download_artifacts(hostname) if options.terminate?
          #if options.terminate?
          #  terminate_instance(instance)
          #end
        end
      end

      def test_impl(tag, hostname, instance, conn, options, image_id=nil)
        begin
          validate_instance(hostname, 4)
          mcollective_logs(hostname) if options.mcollective_logs?
          idle_all_gears(hostname) unless options.official?
          reset_test_dir(hostname, false, options.ssh_user)

          broker_profiler(hostname) if options.profile_broker?

          # Some of the tests expect quickstarts.json to be non-empty
          qs_path = File.expand_path(File.dirname(__FILE__) + "/../misc/devenv/quickstarts.json")
          scp_to(hostname, qs_path, "~/", 60*10, 5, ssh_user)
          ssh(hostname, "sudo mv ~/quickstarts.json /etc/openshift/", 60 * 2, true, 1, options.ssh_user)

          # Some of the tests require that DEFAULT_MAX_GEARS be set to 3
          ssh(hostname, 'sudo sed -i "s/^DEFAULT_MAX_GEARS=.*/DEFAULT_MAX_GEARS=\"3\"/" /etc/openshift/broker-dev.conf; sudo service openshift-broker restart',
              60 * 2, true, 1, options.ssh_user) 

          test_queues = [[], [], [], []]

          extended_tests = nil
          if options.include_extended
            extended_tests = []
            extended_tests = options.include_extended.split(",").map do |extended_test|
              extended_test.strip
            end
          end
 
          # test dependencies only
          ssh(hostname, "sudo yum install -y --disableplugin=priorities ruby193-rubygem-mocha", 60 * 5, true, 1, ssh_user)
          ssh(hostname, "sudo yum install -y ruby193-rubygem-net-scp ruby193-ruby-mysql ruby193-rubygem-fakefs", 60 * 5, true, 1, ssh_user)

          # non-test dependencies that should be required by cartridges
          ssh(hostname, "sudo yum install -y python-virtualenv python27-python-virtualenv python33-python-virtualenv mod_wsgi python27-mod_wsgi python33-mod_wsgi jboss-eap6-modules jboss-eap6-index jbossas-product-eap", 60 * 5, true, 1, ssh_user)

          if options.include_extended
            extended_tests.each do |extended_test|
              case extended_test
              when 'broker'
                (1..4).each do |i|
                  test_queues[i-1] << build_cucumber_command("REST API Group #{i}", ["@broker_api#{i}", "~@not-enterprise"])
                end
                (1..2).each do |i|
                  test_queues[i-1] << build_rake_command("OpenShift Broker Functionals Ext #{i}", "cd /data/openshift-test/broker; rake test:functionals_ext#{i}")
                end
                test_queues[3] << build_rake_command("OpenShift Broker Integration Ext", "cd /data/openshift-test/broker; rake test:integration_ext", {}, true)
                test_queues[3] << build_rake_command("OpenShift Broker OO Admin Scripts", "cd /data/openshift-test/broker; rake test:oo_admin_scripts", {}, true)
              when 'node'
                (1..3).each do |i|
                  test_queues[i-1] << build_cucumber_command("Extended Node Group #{i}", ["@node_extended#{i}", "~@not-enterprise"])
                end
                test_queues[3] << build_rake_command("OpenShift Node Functionals Ext", "cd /data/openshift-test/node; rake ext_node_func_test", {}, true)
              when 'cartridge'
                (1..4).each do |i|
                  test_queues[i-1] << build_cucumber_command("Extended Cartridge Group #{i}", ["@cartridge_extended#{i}", "~@not-enterprise"])
                end
              when 'gear'
                (1..3).each do |i|
                  test_queues[i-1] << build_cucumber_command("Extended Gear Group #{i}", ["@gear_extended#{i}", "~@not-enterprise"])
                end
                (1..3).each do |i|
                  test_queues[i-1] << build_rake_command("OpenShift Gear Functionals Ext #{i}", "cd /data/openshift-test/node; rake ext_gear_func_test#{i}", {}, true)
                end
              when 'rhc'
                test_queues[0] << build_cucumber_command("RHC Integration",["~@not-enterprise"],
                                                         {"RHC_SERVER" => "localhost", "QUIET" => "1"},
                                                         nil,"cucumber", '*.feature', '/data/openshift-test/rhc',
                                                         nil, '/tmp/rhc_bundle')
              else
                puts "Not supported for extended: #{extended_test}"
                exit 1
              end
            end
          elsif options.include_cucumber
            timeout = @@SSH_TIMEOUT
            timeout = @@SSH_TIMEOUT_OVERRIDES[options.include_cucumber] if not @@SSH_TIMEOUT_OVERRIDES[options.include_cucumber].nil?
            test_queues[0] << build_cucumber_command(options.include_cucumber, ["@#{options.include_cucumber}", "~@not-enterprise"], nil, nil, "/data/openshift-test/tests")
          else
            # Run oo-diagnostics on the host 
            # TODO filter out failures we don't care about and fail on others
            out, ret = ssh(hostname, "sudo -s oo-diagnostics", 60 * 20, true, 1, ssh_user)
            print_highlighted_output( "oo-diagnostics", out)
            #oofails = out.scan(/FAIL: /).size
            #if ret != 0 and oofails > 0
              # oofails == 3, since the default password failure is actually listed as 2 (since it is run from oo-accept-broker)
            #  unless oofails == 3 and out.match(/FAIL: test_enterprise_rpms/) and out.match(/FAIL: Datastore Password has been left configured as the default 'mooo'/)
            #    print_and_exit(ret, "oo-diagnostics failed for more than test_enterprise_rpms and default mongo password")
            #  end
            #end

            unless options.exclude_broker?
              test_queues[3] << build_rake_command("OpenShift Broker Units", "cd /data/openshift-test/broker; rake test:units", {}, false)
              test_queues[1] << build_rake_command("OpenShift Broker Integration", "cd /data/openshift-test/broker; rake test:integration", {}, false)
              (1..3).each do |i|
                test_queues[i-1] << build_rake_command("OpenShift Broker Functional #{i}", "cd /data/openshift-test/broker; rake test:functionals#{i}", {}, false)
              end
              test_queues[1] << build_rake_command("OpenShift Admin Console Functional", "cd /data/openshift-test/broker; rake test:admin_console_functionals", {}, false)
              test_queues[3] << build_cucumber_command("Broker Cucumber", ["@broker", "~@not-enterprise"], nil, nil, '/data/openshift-test/tests')
            end

            unless options.exclude_node?
              test_queues[2] << build_rake_command("Node Essentials", "cd /data/openshift-test/node; rake essentials_test", {}, false)
              test_queues[1] << build_rake_command("Node Frontend Plugin ApacheDB", "cd /data/openshift-test/plugins/frontend/apachedb; rake test", {}, false)
              test_queues[2] << build_rake_command("Node Frontend Plugin Apache Mod Rewrite", "cd /data/openshift-test/plugins/frontend/apache-mod-rewrite; rake test", {}, false)
              test_queues[0] << build_rake_command("Node Frontend Plugin Apache Vhost", "cd /data/openshift-test/plugins/frontend/apache-vhost; rake test", {}, false)
              test_queues[3] << build_rake_command("Node Frontend Plugin NodeJS Websocket", "cd /data/openshift-test/plugins/frontend/nodejs-websocket; rake test", {}, false)
              test_queues[2] << build_rake_command("Node Frontend Plugin Haproxy SNI Proxy", "cd /data/openshift-test/plugins/frontend/haproxy-sni-proxy; rake test", {}, false)
              (1..3).each do |i|
                test_queues[i-1] << build_cucumber_command("Node Group #{i.to_s}", ["@node#{i.to_s}", "~@not-enterprise"], nil, nil, '/data/openshift-test/tests')
              end
            end

            unless options.exclude_cartridge?
              #test_queues[0] << build_rake_command("Cartridge Functional", "cd /data/openshift-test/node; rake cart_essentials_test", {}, false)
              (1..3).each do |i|
                test_queues[i-1] << build_cucumber_command("Cartridge Group #{i.to_s}", ["@cartridge#{i.to_s}", "~@not-enterprise"], nil, nil, '/data/openshift-test/tests')
              end
            end

            unless options.exclude_rhc?
              test_queues[0] << build_rake_command("RHC Spec", "cd /data/openshift-test/rhc; bundle install --path=/tmp/rhc_bundle && bundle exec rake spec", {}, false)
              test_queues[0] << build_rake_command("RHC Features", "cd /data/openshift-test/rhc; export TEST_INSECURE=1; export TEST_RANDOM_USER=1; export RHC_SERVER=localhost; bundle install --path=/tmp/rhc_bundle && bundle exec rspec features/*_feature.rb", {}, false)
            end
          end

          run_tests_with_retry(test_queues, hostname, options.ssh_user, ["~@not-enterprise"])

          #These are special tests that cannot be written to work concurrently
          singleton_queue = []
          if options.include_extended
            extended_tests.each do |extended_test|
              case extended_test
                when 'broker'
                when 'node'
                  idle_all_gears(hostname)
                  singleton_queue << build_cucumber_command("Node singletons", ["@singleton", "~@not-enterprise"], nil, nil, '/data/openshift-test/tests')
                when 'gear'
                  idle_all_gears(hostname)
                  singleton_queue << build_cucumber_command("Gear singletons", ["@gear_singleton", "~@not-enterprise"], nil, nil, '/data/openshift-test/tests')
                when 'cartridge'
                when 'site'
                when 'rhc'
                else
                  puts "Not supported for extended: #{extended_test}"
                  exit 1
              end
            end
            run_tests_with_retry([singleton_queue], hostname, options.ssh_user, ["~@not-enterprise"])
          end
          validate_instance(hostname, 4)

          if options.official?
            image_id = image_id ? image_id : instance.image_id
            # Mark the image as verified
            image = conn.images[image_id]
            verify_image(image)

            puts "Sending QE ready email..."
            begin
              send_verified_email(image_id, image.name)
            rescue Exception => e
              puts "Failed sending email with message: #{e.message}"
            end
          elsif !options.terminate?
            idle_all_gears(hostname)
          end

          broker_profiler(hostname, enable=false) if options.profile_broker?

          puts "Done"

        ensure
          # add frontend config for any gears that may be left over from testing
          ssh(hostname, "sudo bash -c \"/usr/bin/oo-frontend-plugin-modify --rebuild --confirm\"", 120, false, 1, options.ssh_user)

          # reset quickstarts.json back to an empty list
          ssh(hostname, 'sudo bash -c echo "[]" > /etc/openshift/quickstarts.json', 60 * 2, true, 1, options.ssh_user) 

          # reset DEFAULT_MAX_GEARS="100"
          ssh(hostname, 'sudo sed -i "s/^DEFAULT_MAX_GEARS=.*/DEFAULT_MAX_GEARS=\"100\"/" /etc/openshift/broker-dev.conf; sudo service openshift-broker restart', 
              60 * 2, true, 1, options.ssh_user) 

          download_artifacts(hostname) if options.terminate?
          if options.terminate?
            terminate_instance(instance)
          end
        end
      end

      def sync_impl(name, options)
        # Get the hostname from a tag lookup or assume it's SSH accessible directly
        hostname = get_host_by_name_or_tag(name, options, ssh_user)

        clone_commands, working_dirs = sync_available_sibling_repos(hostname, options.branch, "/data", ssh_user)
        update_remote_tests(hostname, options.branch, "/data", ssh_user)
        
        if !options.skip_build?
          puts "Performing remote install..."
          last_line = ssh_pty(hostname, %{
# Start shell code

set -e
#{options.clean_metadata? ? 'yum clean metadata' : ''}
cd /data/
rm -rf #{working_dirs}
#{clone_commands}

pushd #{DEV_TOOLS_EXT_REPO} > /dev/null
  echo build/devenv update#{options.verbose? ? ' --verbose' : ''} #{options.clean_metadata? ? ' --include_stale' : ''} | sudo scl enable ruby193 v8314 - 2>&1
popd > /dev/null

#{options.clean_metadata? ? "yum update -y rhc *openshift* 2>&1;" : ''}

# End shell code
echo SUCCESS
}, 900, ssh_user)

          unless last_line =~ /SUCCESS/
            puts "Build failed!  Exiting."
            exit 1
          end

          post_launch_setup(hostname, options)
        end

        puts "Done"
      end
  
      def build_impl(name, build_num, image, conn, options)
        if options.install_required_packages?
          $amz_options[:user_data] = "#!/bin/bash\nset -x\necho Defaults:root,ec2-user \!requiretty >> /etc/sudoers\nsed -i '/cat <<EOL/,$d' /etc/rc.local\nsed -i 's/PermitRootLogin without-passwordUseDNS no/PermitRootLogin without-password\nUseDNS no/g' /etc/ssh/sshd_config\n"
        else
          $amz_options[:user_data] = "#!/bin/bash\nset -x\nsed -i '/cat <<EOL/,$d' /etc/rc.local\nsed -i 's/PermitRootLogin without-passwordUseDNS no//g' /etc/ssh/sshd_config\n"
        end
        puts "Launching AMI: #{image.id} - #{image.name}"
        instance = launch_instance(image, name + "_" + build_num, 1, ssh_user)

        hostname = instance.dns_name

        puts "Building on: #{hostname}"

        begin
          puts "Updating all packages on the system..."
          ssh_pty(hostname, "sudo yum clean metadata; sudo -s yum -y update", 1800, ssh_user)

          if options.install_required_packages?
            puts "Uploading yum client certificates..."
            pem_path = File.expand_path(File.dirname(__FILE__) + "/../misc/client-cert.pem")
            scp_to(hostname, pem_path, "~/", 60*10, 5, ssh_user)
            ssh(hostname, "sudo -s mv ~/client-cert.pem /var/lib/yum/", 60 * 10, true, 1, ssh_user)
            pem_path = File.expand_path(File.dirname(__FILE__) + "/../misc/client-key.pem")
            # needed for init_repos
            scp_to(hostname, pem_path, "~/", 60*10, 5, ssh_user)
            ssh(hostname, "sudo -s mv ~/client-key.pem /var/lib/yum/", 60 * 10, true, 1, ssh_user)
            scp_to(hostname, "misc/devenv/root/.ssh/*", "~/.ssh/", 60*10, 5, ssh_user)
            ssh(hostname, "sudo -s chmod 0600 ~/.ssh/id_rsa; sudo -s chmod 0644 ~/.ssh/id_rsa.pub ~/.ssh/known_hosts;",
                           60*10, true, 1, ssh_user)
            puts "Done"

            puts "Setting up yum repos..."
            script_path = File.expand_path(File.dirname(__FILE__) + "/../misc/devenv/setup-devenv-repos.sh")
            scp_to(hostname, script_path, "~/", 60*10, 5, ssh_user)
            ssh(hostname, "sudo -s /bin/bash ~/setup-devenv-repos.sh #{options.yum_repo};", 1800, true, 1, ssh_user)
            puts "Done"

            puts "Setting up yum priorities..."
            ssh_pty(hostname, "sudo -s yum -y install yum-plugin-priorities", 60 * 20, ssh_user)
            repo = 'rhui-REGION-rhel-server-rhscl'
            ssh_pty(hostname, "sudo -s yum-config-manager --setopt=#{repo}.priority=10 #{repo} --save", 60 * 20, ssh_user)
            repo = 'rhui-REGION-rhel-server-releases'
            ssh_pty(hostname, "sudo -s yum-config-manager --setopt=#{repo}.priority=20 #{repo} --save", 60 * 20, ssh_user)
            ssh_pty(hostname, "sudo -s yum-config-manager --setopt='#{repo}.exclude=tomcat6*' #{repo} --save", 60 * 20, ssh_user)
            ssh_pty(hostname, "sudo -s yum install -y rh-amazon-rhui-client-jbeap6 rh-amazon-rhui-client-jbews2", 60 * 20, ssh_user)
            repos = ['rhui-REGION-jbeap-6-rhui-rhel-6-rpms', 'rhui-REGION-rhel-server-6-jbews2'] 
            repos.each do |repo|
              ssh_pty(hostname, "sudo -s yum-config-manager --setopt=#{repo}.priority=30 #{repo} --save", 60 * 20, ssh_user)
            end
          end

          if options.install_required_packages?
            puts "Configuring Charlie"
            ssh_pty(hostname, "sudo -s yum -y install charlie", 60 * 10, ssh_user)
            ssh(hostname, "sudo -s echo HOURS=6 > /etc/charlie.conf", 60 * 10, true, 1, ssh_user)
          end

          puts "Creating mount..."
          if options.install_from_source? || options.install_from_local_source?
            out, ret = ssh(hostname, "sudo -s rm -rf /data", 60 * 10, true, 1, ssh_user)
            print_and_exit(ret, out) if ret != 0
          end
          out, ret = ssh(hostname, "sudo -s mkdir -p /data && sudo -s chown -R #{SSH_USER}:#{SSH_USER} /data/", 60 * 10, true, 1, ssh_user)
          print_and_exit(ret, out) if ret != 0

          output = ''
          clone_commands = repo_clone_commands(hostname)
          cmd = "set -ex;"
          if options.install_from_source? || options.install_from_local_source?
            if options.install_from_source?
              puts "Performing clean install from source..."
            elsif options.install_from_local_source?
              puts "Performing clean install from local source..."
            end
            init_repos(hostname, true, nil, "/data", ssh_user)
            sync_repos(hostname, "/data", ssh_user) if options.install_from_local_source?
            cmd += %{ pushd /data 
#{clone_commands}
popd
mkdir -p /tmp/tito
}
            wantedbranch = options.install_from_source? ? options.branch : "master"
            SIBLING_REPOS.each_key do |repo_name|
              cmd += "pushd /data/#{repo_name}; git checkout #{wantedbranch}; popd;"
            end
            cmd += %{
pushd /data/#{DEV_TOOLS_EXT_REPO} > /dev/null
  echo "Building all packages on the server..."
  sudo -s scl enable ruby193 v8314 "build/devenv local_build --clean-packages"
popd > /dev/null
}
          elsif options.install_required_packages?
            puts "Installing bootstrap packages..."
            output, exit_code = ssh(hostname, "sudo -s yum install -y ruby193-ruby ruby193-rubygem-thor ruby193-rubygem-capybara ruby193-rubygem-ci_reporter ruby193-rubygem-minitest ruby193-rubygem-mocha ruby193-rubygem-poltergeist ruby193-rubygem-simplecov ruby193-rubygem-webmock ruby193-net-scp ruby193-rubygem-net-ssh-multi ruby193-rubygem-net-ssh-gateway ruby193 scl-utils ruby193-rubygem-aws-sdk ruby193-rubygem-parseconfig ruby193-rubygem-archive-tar-minitar", 60 * 20, true, 1, ssh_user)

            #output, exit_code = ssh(hostname, "yum -y install git tito ruby rubygems rubygem-thor rubygem-parseconfig rubygem-json rubygem-aws-sdk ruby193-rubygem-thor ruby193-rubygem-parseconfig ruby193-rubygem-json ruby193-rubygem-aws-sdk scl-utils-build createrepo yum-priorities", 600, true)
            print_highlighted_output('Install Bootstrap Packages Output', output)
            exit 1 unless exit_code == 0
            puts "Install tito..."
            output, exit_code = ssh(hostname, "sudo yum install -y --enablerepo=epel --disablerepo=Node tito", 60 * 15, true, 1, ssh_user) if ret == 0
            print_highlighted_output('Install tito Output', output)
            exit 1 unless exit_code == 0
            puts "Install puppet..."
            output, exit_code = ssh(hostname, "sudo yum install -y --enablerepo=puppetlabs-products --disableplugin=priorities puppet-3.1.1-1.el6", 60 * 15, true, 1, ssh_user) if ret == 0
            print_highlighted_output('Install puppet Output', output)
            exit 1 unless exit_code == 0
            puts "Done"

            puts "Installing requires..."
            init_repos(hostname, true, nil, "/data", ssh_user)
            cmd += %{ pushd /data
#{clone_commands} 
popd
}
            SIBLING_REPOS.each_key do |repo_name|
              cmd += "pushd /data/#{repo_name}; git checkout #{options.branch}; popd;"
            end
            cmd += %{
pushd /data/#{DEV_TOOLS_EXT_REPO}
  sudo -s scl enable ruby193 v8314 "build/devenv install_required_packages" 2>&1
popd
sudo -s rm -rf /etc/puppet/modules &&
sudo -s mkdir -p /etc/puppet/modules &&
sudo -s puppet module install puppetlabs/stdlib &&
sudo -s puppet module install puppetlabs/ntp &&
sudo -s puppet module install rharrison/lokkit &&
sudo -s puppet module install blentz/selinux_types &&
sudo -s puppet module install puppetlabs/haproxy &&
sudo -s puppet module install arioch/keepalived &&
sudo -s puppet module install duritong/sysctl &&
sudo -s ln -sf /data/puppet-openshift_origin /etc/puppet/modules/openshift_origin
}
          else
            puts "Performing clean install with the last published rpms..."
          end
          cmd += "echo SUCCESS"

          puts 'Install Output'
          last_line = ssh_pty(hostname, cmd, 3600, ssh_user)
          puts "Done"
          exit 1 unless last_line =~ /SUCCESS/

          # Cleanup any lines that break sshd_config (cloud-init RHEL 6.4 bug)
          puts "Cleaning up broken sshd_config (RHEL 6.4 cloud-init bug)"
          ssh(hostname, 'sudo -s sed -i "s/PermitRootLogin without-passwordUseDNS no//g" /etc/ssh/sshd_config', 60, true, 1, ssh_user)

          output = ssh(hostname, "sudo yum list installed", 120, true, 1, ssh_user)
          print_highlighted_output('Installed Packages', output)

          image_id = nil
          if options[:register]
            # reset the eth0 network config to remove the HWADDR
            puts "Removing HWADDR and DNS entries from eth0 network config..."
            reset_eth0_dns_config(hostname)
            
            manifest = rpm_manifest(hostname, ssh_user)              
            registered_ami = register_image(conn, instance, name + '_' + build_num, manifest)
            image_id = registered_ami.id
          end

          unless options.install_required_packages?
            puts "Running post_launch_setup..."
            post_launch_setup(hostname, options)
          end

          unless options.skip_verify? || options.install_required_packages?
            wantedbranch = options.install_from_local_source? ? "master" : options.branch
            update_remote_tests(hostname, wantedbranch, "/data", options.ssh_user)
            test_impl(name + '_' + build_num, hostname, instance, conn, options, image_id)
          end
        ensure
          begin
            terminate(name + '_' + build_num) if options.terminate?
          rescue
            # suppress termination errors - they have been logged already
          end
        end
      end

      def reset_eth0_dns_config(hostname)
cmd = %{
sudo -s echo \\\"DEVICE=eth0
BOOTPROTO=dhcp
ONBOOT=yes
\\\" > /etc/sysconfig/network-scripts/ifcfg-eth0

sudo -s /etc/init.d/network restart
sudo -s service named restart
}
        out, ret = ssh(hostname, cmd, 60 * 5, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0
      end
  
      def sanity_check_impl(tag, hostname, instance, conn, options, image_id=nil)
        threads = []
        failures = []
        titles = ["OpenShift Origin Broker Sanity",
                  "OpenShift Origin Node Unit"]
    
        cmds = ["cd /var/www/openshift/broker; sudo -s bundle exec rake test:sanity",
                "cd /data/openshift-test/node; sudo -s rake unit_test"]
        add_ssh_cmd_to_threads(hostname, threads, failures, titles, cmds, false, @@SSH_TIMEOUT, ssh_user)
        add_ssh_cmd_to_threads(hostname, threads, failures, "Cucumber Sanity", "cucumber #{CUCUMBER_OPTIONS} -t @sanity /data/openshift-test/tests/", false, @@SSH_TIMEOUT, ssh_user)
    
        threads.each do |t|
          t[0].join
        end
    
        unless failures.empty?
          failures.uniq!
          retry_test_failures(hostname, failures, 1, @@SSH_TIMEOUT, ssh_user)
        end
      end
      
      def update_facts_impl(hostname)
      end

      def update_puppet_manifests(hostname, options, authtype)
        out, ret = ssh(hostname, %{
cat<<EOF > ~#{options.ssh_user}/configure_hostname.pp
file { "update network settings - hostname": 
  path    => "/etc/sysconfig/network",
  content => "NETWORKING=yes\nNETWORKING_IPV6=no\nHOSTNAME=\\\${ec2_public_hostname}\n"
}
exec { "set hostname":
  command => "/bin/hostname \\\${ec2_public_hostname}"
}
EOF
        }, 60 * 2, true, 3, options.ssh_user)
        print_and_exit(ret, out) if ret != 0
        out, ret = ssh(hostname, %{
cat<<EOF > ~#{options.ssh_user}/configure_origin.pp
\\\$keyfile="/var/named/Kdev.rhcloud.com.*.key"
\\\$key=inline_template("<%=File.read(Dir.glob(keyfile)[0]).strip.split(\' \')[7]%>")
class { "openshift_origin" :
  node_hostname                      => \\\$ec2_public_hostname,
  domain                             => "dev.rhcloud.com",
  conf_nameserver_upstream_dns       => ["172.16.0.23"],
  bind_key                           => \\\$key,
  register_host_with_nameserver      => true,
  node_unmanaged_users               => ["#{options.ssh_user}"],
  install_method                     => 'none',
  development_mode                   => true,
  configure_ntp                      => false,
  conf_broker_session_secret         => "devenv_secret",
  conf_console_session_secret        => "devenv_secret",
  conf_valid_gear_sizes              => ['small', 'medium'],
  conf_broker_multi_haproxy_per_node => true,
  node_frontend_plugins              => ['apache-mod-rewrite', 'nodejs-websocket',
                                         'haproxy-sni-proxy'],
  install_cartridges                 => ['cron','diy','haproxy','mongodb',
                                         'nodejs','perl','php','postgresql',
                                         'python','ruby','jenkins','jenkins-client',
                                         'mysql', 'phpmyadmin'],
}
EOF
        }, 60 * 2, true, 3, options.ssh_user)
        print_and_exit(ret, out) if ret != 0
      end
      
      def post_launch_setup(hostname, options)                        
        puts "Updating puppet manifests"
        update_puppet_manifests(hostname, options, "basic-auth")

        # Configure httpd virtual host for binary artifacts
        # Needed for binary deployment tests
        puts "Configuring Virtual Host for binary artifact deployment tests"
        out, ret = ssh(hostname, "sudo -s mkdir -p /var/www/html/binaryartifacts; sudo chmod 775 /var/www/html/binaryartifacts", 60 * 10, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0
        binartifacts_path = File.expand_path(File.dirname(__FILE__) + "/../misc/binaryartifacts.conf")
        scp_to(hostname, binartifacts_path, "~/", 60*10, 5, ssh_user)
        out, ret = ssh(hostname, "sudo -s mv binaryartifacts.conf /etc/httpd/conf.d; sudo restorecon /etc/httpd/conf.d; sudo service httpd restart", 60 * 20, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0

        # reset the eth0 network config to add the dns entries
        puts "Running puppet - configure_hostname.pp"
        ssh_pty(hostname, "sudo -s puppet apply --verbose ~#{SSH_USER}/configure_hostname.pp", 60 * 20, ssh_user)

        puts "Cleaning up old dnssec keys"
        out = ssh(hostname, 'sudo find /var/named -name Kdev.rhcloud.com* -exec rm {} \;', 60 * 20, false, 1, ssh_user)
        print_highlighted_output( "Cleaning up old dnssec keys", out)

        puts "Configuring dnssec keys"
        out, ret = ssh(hostname, 'sudo -s yum install -y bind', 60 * 20, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0
        out, ret = ssh(hostname, 'sudo -s /usr/sbin/dnssec-keygen -a HMAC-MD5 -b 512 -n USER -r /dev/urandom -K /var/named dev.rhcloud.com', 60 * 20, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0
        out, ret = ssh(hostname, 'TSIG_KEY=`sudo -s cat /var/named/Kdev.rhcloud.com.*.key | awk "{ print $8 }"` ; echo $TSIG_KEY', 60 * 20, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0

        puts "Cleanup mongo setup marker"
        out, ret = ssh(hostname, 'sudo -s rm -f /etc/openshift/.mongo-setup-complete', 60 * 20, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0
        puts "Running puppet - configure_origin.pp"
        ssh_pty(hostname, "sudo -s puppet apply --verbose ~#{SSH_USER}/configure_origin.pp", 60 * 30, ssh_user)

        # Overwrite iptables rules after puppet run
        puts "Cleanup iptables rules"
        iptables_path = File.expand_path(File.dirname(__FILE__) + "/../misc/iptables")
        scp_to(hostname, iptables_path, "~/", 60*10, 5, ssh_user)
        out, ret = ssh(hostname, "sudo -s mv iptables /etc/sysconfig/iptables; sudo restorecon /etc/sysconfig/iptables; sudo service iptables restart", 60 * 20, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0

        # TODO: activemq is failing to start or crashing
        puts "Restart Services"
        out, ret = ssh(hostname, "sudo -s service activemq restart; sleep 10; sudo -s service ruby193-mcollective restart", 60 * 20, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0
        print_highlighted_output( "Service Restart", out)
        restart_services_remote(hostname)

        # ensure all cartridges are recognized by the broker
        puts "Add cartridges to mongo"
        out, ret = ssh(hostname, "sudo -s oo-admin-ctl-cartridge -c import-node --activate --obsolete", 60 * 20, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0
        out, ret = ssh(hostname, "sudo -s RAILS_ENV=test oo-admin-ctl-cartridge -c import-node --activate --obsolete", 60 * 20, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0

        # create default district
        puts "Creating default district (if it doesn't already exist)"
        out, ret = ssh(hostname, "sudo -s oo-admin-ctl-district -p small -n default-small -c add-node --available", 60 * 20, true, 1, ssh_user)
        print_highlighted_output( "Create district", out)

        # install openshift-origin-cartridge-dependencies-optional-php
        puts "Installing openshift-origin-cartridge-dependencies-optional-php"
        out, ret = ssh(hostname, "sudo -s yum -y install openshift-origin-cartridge-dependencies-optional-php", 60 * 20, true, 1, ssh_user)

      end

      def restart_services
        cmd = %{
        sudo -s service activemq restart; sudo -s service mongod restart
        sudo -s service cgconfig restart; sudo -s service cgred restart ; sudo -s service openshift-cgroups restart
        sudo -s service httpd restart; sudo -s service openshift-broker restart
        sudo -s service openshift-console restart; 
        sudo -s service openshift-node-web-proxy stop; 
        sudo -s service named restart; sudo -s service network restart; sudo -s service sshd restart
        sudo -s service ruby193-mcollective restart; sudo -s service oddjobd restart
        sudo -s service openshift-node-web-proxy start;
        sudo -s service openshift-iptables-port-proxy restart; sudo -s service openshift-tc restart
        }
        run(cmd)
      end

      def restart_services_remote(hostname)
        puts "Restarting services..."

        cmd = %{
sudo -s service activemq restart; sudo -s service mongod restart
sudo -s service httpd restart; sudo -s service openshift-broker restart
sudo -s service openshift-console restart; 
sudo -s service openshift-node-web-proxy stop;
sudo -s service named restart; sudo -s service network restart; sudo -s service sshd restart
sudo -s service cgconfig restart; sudo -s service cgred restart ; sudo -s service openshift-cgroups restart
sudo -s service ruby193-mcollective restart; sudo -s service oddjobd restart
sudo -s service openshift-node-web-proxy start;
sudo -s service openshift-iptables-port-proxy restart; sudo -s service openshift-tc restart
}
        out, ret = ssh(hostname, cmd, 60 * 5, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0
        puts "Done"
      end
    end # no_tasks end
  end # class end
end # module end
Origin::BuilderPlugin.start
