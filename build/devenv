#!/usr/bin/env ruby

$: << File.expand_path(File.dirname(__FILE__))
require 'origin_constants'
#unless ENV['SKIP_SETUP']
#  require 'lib/openshift/setup_helper'
#  SetupHelper::ensure_build_requirements
#end
require File.join('lib', '..', '..', '..', "#{DEV_TOOLS_REPO}", 'build', 'lib', 'openshift')
require File.join('lib', '..', '..', '..', "#{DEV_TOOLS_REPO}", 'build', 'builder')
require 'rubygems'
require 'thor'
require 'fileutils'
require 'lib/openshift'
require 'pp'
require 'yaml'
require 'builder'

include FileUtils

module Origin
  class BuilderPlugin < OpenShift::Builder
    include OpenShift::BuilderHelper

    desc "build_livecd", "Build a livecd"
    def build_livecd
      basedir = "/root"
      remix_dir = "/root/origin-server/remix"
      remix_ks = "openshift-origin-remix.ks"  
      
      FileUtils.mkdir_p remix_dir
      git_rev = `git log --pretty="format:%H %cd" -1`
      
      system "rm -f #{remix_dir}/#{remix_ks}"
      ks_data = File.read("/usr/share/openshift/kickstarts/#{remix_ks}").gsub(/#ADDITIONAL REPOS/, "repo --name=local-build --baseurl=file://#{basedir}/origin-rpms\n#ADDITIONAL REPOS")
      ks_data.gsub!(/#GIT_REV#/,git_rev)
      
      if File.exist?("#{basedir}/extras")
        system "createrepo #{basedir}/extras"
        ks_data.gsub!(/#ADDITIONAL REPOS/, "repo --name=local-extras --baseurl=file://#{basedir}/extras\n#ADDITIONAL REPOS")
        #ks_data.gsub!(/#cartridge/,"cartridge")
      end
      File.open("#{remix_dir}/#{remix_ks}", 'w') do |out|
        out << ks_data
      end
      
      run "/sbin/service mongod stop"
      run "/usr/sbin/setenforce 0"
      run "cd #{remix_dir} && livecd-creator -c openshift-origin-remix.ks -f openshift_origin --cache=cache -d -v --logfile=livecd.log"
      run "/usr/sbin/setenforce 1"
      run "/sbin/service mongod start"
    end

    no_tasks do
      alias_method :old_install_required_packages, :install_required_packages
    end

    def install_required_packages
      base_os = guess_os
      puts "Install packages required for build"
      
      if `rpm -q activemq`.match(/is not installed/)
        run "yum erase -y activemq"
        if base_os == "fedora"
          run "yum install -y https://mirror.openshift.com/pub/origin-server/fedora-18/x86_64/activemq-5.6.0-4.fc18.x86_64.rpm"
          run %{ yum erase -y mcollective mcollective-common mcollective-client;\
                 yum install -y yum update -y https://mirror.openshift.com/pub/origin-server/fedora-17/x86_64/mcollective-2.2.0-2.fc17.noarch.rpm \
                 https://mirror.openshift.com/pub/origin-server/fedora-17/x86_64/mcollective-common-2.2.0-2.fc17.noarch.rpm \
                 https://mirror.openshift.com/pub/origin-server/fedora-17/x86_64/mcollective-client-2.2.0-2.fc17.noarch.rpm;
              }
        elsif(base_os == "rhel" or base_os == "centos")
          run "yum install -y activemq"
        end
      end
      
      if base_os == "fedora"
        run "yum -y update --enablerepo updates-testing ruby ruby-irb ruby-libs ruby-devel"
        run("gem install rspec -v '1.1.12'", options)
      end

      run "wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat/jenkins.repo"
      run "rpm --import http://pkg.jenkins-ci.org/redhat/jenkins-ci.org.key"      
      run "yum install -y tito make tig mlocate bash-completion activemq-client"

      old_install_required_packages
    end
    
    desc "local_build", "Builds and installs all packages locally"
    method_option :base_os, :default => nil, :desc => "Operating system for Origin (fedora or rhel)"
    method_option :verbose, :type => :boolean, :desc => "Enable verbose logging"
    method_option :clean_packages, :type => :boolean, :desc => "Erase existing packages before install?"
    method_option :update_packages, :type => :boolean, :desc => "Run yum update before install?"
    method_option :incremental, :type => :boolean, :desc => "Build only the changed packages"
    def local_build
      options.verbose? ? @@log.level = Logger::DEBUG : @@log.level = Logger::ERROR
      def_constants(guess_os(options.base_os))
      
      if options.incremental
        options.retry_failure_with_tag = false
        update
      else
        FileUtils.rm_f "/etc/yum.repos.d/local-openshift-origin.repo"
        FileUtils.rm_rf "/tmp/tito"
        
        packages = get_packages(false, true).values

        if options.clean_packages?
          run("yum clean all", options)
          package_names = "\"#{packages.join("\" \"")}\""
          puts "Removing stale packages..."
          run("yum erase -y #{package_names}", options)
        end
        
        if options.update_packages?
          puts "Updating all packages on the system..."
          run("yum update -y", options)
          puts "Done"
        end

        find_and_build_specs

        FileUtils.rm_rf "/data/origin-rpms"
        FileUtils.rm_rf "/data/origin-srpms"
        FileUtils.mkdir_p "/data/origin-rpms"
        FileUtils.mkdir_p "/data/origin-srpms"        
        File.open("/etc/yum.repos.d/local-openshift-origin.repo", 'w') do |out|
          out << %{
[openshift-origin]
name    = openshift-origin
baseurl = file:///data/origin-rpms
gpgcheck= 0
enabled = 1
retries = 0
priority= 1
          }
        end
        run("cp /tmp/tito/x86_64/*.rpm /data/origin-rpms/; cp /tmp/tito/noarch/*.rpm /data/origin-rpms/; createrepo /data/origin-rpms/", options)
        run("cp /tmp/tito/*.src.rpm /data/origin-srpms/; chown -R #{SSH_USER}:#{SSH_USER} /data/origin-srpms", options)
        run("yum clean all")
        
        packages_to_install = packages.select{ |p| not IGNORE_PACKAGES.include?(p.name) }
        package_list = "\"#{packages_to_install.join("\" \"")}\""
        run("yum install -y #{package_list}", options)

        #mark all packages as sync'd
        get_sync_dirs
      end
    end


    no_tasks do
      def ssh_user
        return SSH_USER
      end

      def download_artifacts(hostname)
        puts "Downloading logs and screenshots..."
        `rm -rf rhc/log origin-rpms origin-srpms; mkdir -p rhc/log/; mkdir -p origin-rpms; mkdir -p origin-srpms; pushd rhc/log > /dev/null; mkdir -p broker mcollective system screenshots selenium jbossas broker-profiler coverage; popd > /dev/null`
        scp_from(hostname, "/tmp/rhc/*", "rhc/log", 60, ssh_user)
        scp_from(hostname, "/var/log/openshift/broker/*", "rhc/log/broker", 60, ssh_user)
        scp_from(hostname, "/var/log/openshift/user_action.log", "rhc/log/broker/user_action.log", 60, ssh_user)
        scp_from(hostname, "/var/log/mcollective.*", "rhc/log/mcollective", 60, ssh_user)
        scp_from(hostname, "/var/log/httpd/access_log", "rhc/log/system/access_log.log", 60, ssh_user)
        scp_from(hostname, "/var/log/httpd/error_log", "rhc/log/system/error_log.log", 60, ssh_user)
        scp_from(hostname, "/var/log/yum.log", "rhc/log/system/yum.log", 60, ssh_user)
        scp_from(hostname, "/var/log/messages", "rhc/log/system/messages.log", 60, ssh_user)
        scp_from(hostname, "/var/log/secure", "rhc/log/system/secure.log", 60, ssh_user)
        scp_from(hostname, "/var/log/audit/audit.log", "rhc/log/system/audit.log", 60, ssh_user)
        scp_from(hostname, "/tmp/rhc/*_coverage", "rhc/log/coverage", 60, ssh_user)

        ssh(hostname, "sudo -s
          rm -rf /tmp/origin-rpms;
          mkdir -p /tmp/origin-rpms;
          cp -rf /data/origin-rpms/*.rpm /tmp/origin-rpms/;
          chown #{SSH_USER}:#{SSH_USER} -R /tmp/origin-rpms", 60, false, 2, ssh_user)
        scp_from(hostname, "/tmp/origin-rpms/*.rpm", "origin-rpms", 300, ssh_user)
        scp_from(hostname, "/data/origin-srpms/*.src.rpm", "origin-srpms", 300, ssh_user)
        puts "Done"
      end
      
      def validate_instance(hostname, num_tries=1)
      end
      
      def update_cucumber_tests(hostname, repo_parent_dir="/data", user="root")
        ssh(hostname, "
          rm -f #{repo_parent_dir}/openshift-test/tests; 
          ln -s #{repo_parent_dir}/openshift-test/controller/test/cucumber #{repo_parent_dir}/openshift-test/tests ;", 60, false, 2, user)
      end

      def setup_verifier(hostname, branch)
        update_remote_tests(hostname, branch, "/data", ssh_user)
      end

      def rpm_manifest(hostname, sshuser="root")
        print "Retrieving RPM manifest.."
        manifest = ssh(hostname, 'rpm -qa | grep -E "(rhc|openshift)" | grep -v cartridge', 60, false, 1, sshuser)
        manifest = manifest.split("\n").sort.join(" / ")
        # Trim down the output to 255 characters
        manifest.gsub!(/rubygem-([a-z])/, '\1')
        manifest.gsub!('openshift-origin-', '')
        manifest.gsub!('mcollective-', 'mco-')
        manifest.gsub!('.fc16', '')
        manifest.gsub!('.noarch', '')
        manifest.gsub!(/\.git\.[a-z0-9\.]+/, '')
        manifest = manifest[0..254]
        puts "Done"
        return manifest
      end

      def wrap_test_command(command)
        if BASE_OS == "fedora"
          "sudo -s #{command}"
        elsif(BASE_OS == "rhel" or BASE_OS == "centos")
          "sudo -s /usr/bin/scl enable ruby193 \"#{command}\""
        end
      end

      def idle_all_gears(hostname)
        puts "Idling all gears on remote instance: #{hostname}"
        ssh(hostname, "sudo bash -c /sbin/service mcollective stop; /sbin/service mcollective start;", 240, false, 1, options.ssh_user)
        ssh(hostname, 'for dir in /var/lib/openshift/*; do if [ -d $dir ] && [ ! -h $dir ] ; then sudo bash -c "oo-idler -n -u `basename $dir`"; fi; done;', 240, false, 1, options.ssh_user)
        ssh(hostname, 'sudo bash -c "/sbin/service httpd graceful"', 240, false, 1, options.ssh_user)
        puts "Done"
      end

      def test_impl(tag, hostname, instance, conn, options, image_id=nil)
        begin
          
          validate_instance(hostname, 4)
          mcollective_logs(hostname) if options.mcollective_logs?
          reset_test_dir(hostname, false, SSH_USER)

          # Verify we have the mocha gem from openshift-origin-deps
          ssh(hostname, "sudo -s yum install -y --disableplugin=priorities ruby193-rubygem-mocha", 60 * 20, true, 1, ssh_user)

          # Verify we have a version of scl-utils that isn't subject to the 
          # container in container bug: https://bugzilla.redhat.com/show_bug.cgi?id=955669
          # 20120927-8 exhibits the same behavior, due to the X_SCLS environment variable 
          # not being unset by oo-su
          out, ret = ssh(hostname, "rpm -q scl-utils | cut -d '.' -f 1 | cut -d '-' -f 3,4", 60 * 1, true, 1, ssh_user)
          if out.match /^20120927-[0-6]$/
            ssh(hostname, "sudo -s yum install -y --disableplugin=priorities scl-utils-20120927-7.el6.x86_64 scl-utils-build-20120927-7.el6.x86_64",
                60 * 20, true, 1, ssh_user)
          elsif out.match /^20120927-8$/
            ssh(hostname, "sudo -s yum downgrade -y --disableplugin=priorities scl-utils-20120927-7.el6.x86_64 scl-utils-build-20120927-7.el6.x86_64",
                60 * 20, true, 1, ssh_user)
          end

          # Run oo-diagnostics on the host 
          # TODO filter out failures we don't care about and fail on others
          out, ret = ssh(hostname, "sudo -s oo-diagnostics", 60 * 20, true, 1, ssh_user)
          print_highlighted_output( "oo-diagnostics", out)
          oofails = out.scan(/FAIL: /).size
          if ret != 0 and oofails > 0
            unless oofails == 1 and out.match(/FAIL: test_enterprise_rpms/)
              print_and_exit(ret, "oo-diagnostics failed for more than test_enterprise_rpms")
            end
          end

          test_queues = [[], [], [], []]

          ssh(hostname, wrap_test_command("echo 'export REGISTER_USER=1' > /etc/profile.d/testenv.sh") , 60, false, 2, SSH_USER)
          ssh(hostname, wrap_test_command("echo 'export RHC_DOMAIN=example.com' >> /etc/profile.d/testenv.sh"), 60, false, 2, SSH_USER)
          ssh(hostname, wrap_test_command("echo 'export RHC_SERVER=#{hostname}' >> /etc/profile.d/testenv.sh"), 60, false, 2, SSH_USER)
          # TODO: is this needed?
          # ssh(hostname, "oo-register-user -ladmin -padmin --username user_with_multiple_gear_sizes@test.com --userpass test" , 60, false, 2, SSH_USER)
          # ssh(hostname, "oo-register-user -ladmin -padmin --username user_with_extra_storage@test.com --userpass test" , 60, false, 2, SSH_USER)
            
          extended_tests = nil
          if options.include_extended
            extended_tests = []
            extended_tests = options.include_extended.split(",").map do |extended_test|
              extended_test.strip
            end
          end

          # OSE 1.2 is focussed on the v2 cucumber tests right now.  These are
          # all in the runtime extended tests.  Jenkins doesn't run these
          # by default so we have explicitly made them the default.
          ssh(hostname, "sudo -s mkdir -p /var/lib/openshift/.settings; sudo -s touch /var/lib/openshift/.settings/v2_cartridge_format;",
                       60, false, 2, SSH_USER)
          ssh(hostname, "sudo -s /usr/sbin/oo-admin-broker-cache -c --console", 60, false, 2, SSH_USER)
          #test_queues[0] << ["Extended Runtime Other Group 1", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @runtime_extended1 /data/openshift-test/tests")]
          #test_queues[1] << ["Extended Runtime Other Group 2", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @runtime_extended2 /data/openshift-test/tests")]
          #test_queues[2] << ["Extended Runtime Other Group 3", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @runtime_extended3 /data/openshift-test/tests")]
          test_queues[0] << ["Runtime Unit", "cd /data/openshift-test/node;" + wrap_test_command("rake test")]
          (1..4).each do |i|
            test_queues[3] << ["Runtime Group #{i.to_s}", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @runtime#{i.to_s} /data/openshift-test/tests")]
          end

          if options.include_extended
            extended_tests.each do |extended_test|
              case extended_test
              when 'broker'
                test_queues[0] << ["REST API Group 1", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @broker_api1 /data/openshift-test/tests"), {:retry_individually => true}]
                # TODO: BROKEN
                #test_queues[1] << ["REST API Group 2", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @broker_api2 /data/openshift-test/tests"), {:retry_individually => true}]
                # TODO: BROKEN
                #test_queues[2] << ["REST API Group 3", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @broker_api3 /data/openshift-test/tests"), {:retry_individually => true}]
                test_queues[3] << ["REST API Group 4", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @broker_api4 /data/openshift-test/tests"), {:retry_individually => true}]
                # TODO: BROKEN
                #test_queues[2] << ["OpenShift Broker Functionals Ext", "cd /data/openshift-test/broker;" + wrap_test_command("bundle exec rake test:functionals_ext"), {:retry_individually => true}]
              when 'runtime', 'node'
                # TODO: BROKEN blocked on cartridge work
                #test_queues[0] << ["Extended Runtime Group 1", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @runtime_extended1 /data/openshift-test/tests")]
                # TODO: BROKEN mostly blocked on cartridge work, php tests failing too
                #test_queues[1] << ["Extended Runtime Group 2", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @runtime_extended2 /data/openshift-test/tests")]
                # TODO: BROKEN mostly blocked on cartridge work, technically the descriptor.feature passes
                #test_queues[2] << ["Extended Runtime Group 3", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @runtime_extended3 /data/openshift-test/tests")]
              when 'runtime_other', 'cartridge'
                ssh(hostname, "sudo -s mkdir -p /var/lib/openshift/.settings; sudo -s touch /var/lib/openshift/.settings/v2_cartridge_format;",
                    60, false, 2, SSH_USER)
                ssh(hostname, "sudo -s /usr/sbin/oo-admin-broker-cache -c --console", 60, false, 2, SSH_USER)
                test_queues[0] << ["Extended Runtime Other Group 1", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @runtime_extended_other1 /data/openshift-test/tests")]
                #test_queues[1] << ["Extended Runtime Other Group 2", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @runtime_extended_other2 /data/openshift-test/tests")]
                test_queues[2] << ["Extended Runtime Other Group 3", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @runtime_extended_other3 /data/openshift-test/tests")]
                test_queues[0] << ["Runtime Unit", "cd /data/openshift-test/node;" + wrap_test_command("rake test")]
                (1..4).each do |i|
                  test_queues[3] << ["Runtime Other Group #{i.to_s}", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @runtime_other_#{i.to_s} /data/openshift-test/tests")]
                end
                test_queues[3] << ["Runtime Other Group", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @runtime_other /data/openshift-test/tests")]
              when 'rhc'
                # TODO: BROKEN blocked on cartridge work
                #test_queues[0] << ["RHC Extended", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @rhc_extended /data/openshift-test/tests"), {:retry_individually => true}]
                # TODO: BROKEN /data/openshift-test/rhc doesn't even exist
                # test_queues[1] << ["RHC Integration", "cd /data/openshift-test/rhc && " + wrap_test_command("RHC_SERVER=#{hostname} QUIET=1 bundle exec \\\"cucumber #{CUCUMBER_OPTIONS} features\\\""), {:retry_individually => true}]
              else
                puts "No extended tests available for: #{extended_test}"
              end
            end
          elsif options.include_cucumber
            timeout = @@SSH_TIMEOUT
            timeout = @@SSH_TIMEOUT_OVERRIDES[options.include_cucumber] if not @@SSH_TIMEOUT_OVERRIDES[options.include_cucumber].nil?
            test_queues[0] << [options.include_cucumber, wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @#{options.include_cucumber} /data/openshift-test/tests"), {:timeout => timeout}]
          elsif options.include_web?
            puts "No tests available for web"
          else
            unless options.exclude_broker?
              #test_queues[1] << ["OpenShift Broker Units", "cd /data/openshift-test/broker;" + wrap_test_command("rake test:units")]
              
              # Remove sub_user_test from broker integration tests, only works with mongo plugin
              ssh(hostname, "rm /data/openshift-test/broker/test/integration/sub_user_test.rb", 60, false, 2, SSH_USER)

              test_queues[0] << ["OpenShift Broker Integration", "cd /data/openshift-test/broker; sudo -s cp /var/www/openshift/broker/Gemfile.lock . ;" + wrap_test_command("bundle install --local") + ";" + wrap_test_command("bundle exec rake test:integration")]
              # TODO: BROKEN
              #test_queues[2] << ["OpenShift Broker Functional", "cd /data/openshift-test/broker; cp /var/www/openshift/broker/Gemfile.lock ." + wrap_test_command("bundle install --local") + ";" + wrap_test_command("bundle exec rake test:functionals")]
              # TODO: BROKEN
              #test_queues[3] << ["Broker Cucumber", wrap_test_command("cucumber #{BROKER_CUCUMBER_OPTIONS} -t @broker /data/openshift-test/tests")]
            end

            unless options.exclude_runtime?
              #test_queues[0] << ["Runtime Unit", "cd /data/openshift-test/node;" + wrap_test_command("rake unit_test")]
# TODO: BROKEN blocked by cartridge work
=begin
              (1..4).each do |i|
                test_queues[i-1] << ["Runtime Group #{i.to_s}", wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @runtime#{i.to_s} /data/openshift-test/tests")]
              end
=end
            end
  
            unless options.exclude_rhc?
# TODO: BROKEN /data/openshift-test/rhc doesn't exist
=begin
              test_queues[0] << ["RHC Spec", "cd /data/openshift-test/rhc;" + wrap_test_command("bundle install --path=/tmp/rhc_bundle") +" && " + wrap_test_command("bundle exec rake spec")]
=end
            end
          end
  
          threads = []
          failures = []
  
          retry_threshold = 0
          test_queues.each do |test_queue|
            titles = []
            cmds = []
            retry_individually = []
            timeouts = []
            test_queue.each do |test|
              titles << test[0]
              cmds << test[1]
              opts = test[2] || {}
              retry_individually << opts[:retry_individually] ? true : false
              timeouts << opts[:timeout] ? opts[:timeout] : @@SSH_TIMEOUT
              retry_threshold += 8
            end
            add_ssh_cmd_to_threads(hostname, threads, failures, titles, cmds, retry_individually, timeouts, ssh_user)
          end
  
          threads.each do |t|
            t[0].join
          end
 
          failures.uniq!
  
          begin
            if failures.length > 0 && failures.length <= retry_threshold 
              idle_all_gears(hostname)
              retry_test_failures(hostname, failures, 2, timeout=@@SSH_TIMEOUT, ssh_user)
            elsif failures.length > retry_threshold
              exit 1
            end

            # These are special tests that cannot be written to work concurrently
            if options.include_extended
              extended_tests.each do |extended_test|
                case extended_test
                when 'broker'
                when 'runtime', 'runtime_other', 'node'
                  idle_all_gears(hostname)
                  singleton_queue = ['Singletons', wrap_test_command("cucumber #{CUCUMBER_OPTIONS} -t @singleton /data/openshift-test/tests")]
                  output, exit_code = run_ssh(hostname, singleton_queue[0], singleton_queue[1], timeout=@@SSH_TIMEOUT, ssh_user)
                  retry_test_failures(hostname, [singleton_queue], 2, timeout=@@SSH_TIMEOUT, ssh_user) if 0 != exit_code
                when 'site'
                when 'cartridge'
                when 'rhc'
                else
                  puts "Not supported for extended: #{extended_test}"
                  exit 1
                end
              end
            end
  
            validate_instance(hostname, 4)
          end
  
          if options.official?
            image_id = image_id ? image_id : instance.image_id
            # Mark the image as verified
            image = conn.images[image_id]
            verify_image(image)
  
            puts "Sending QE ready email..."
            begin
              send_verified_email(image_id, image.name)
            rescue Exception => e
              puts "Failed sending email with message: #{e.message}"
            end
          end
  
          puts "Done"
  
        ensure
          download_artifacts(hostname)
          
          begin
            terminate(tag) if options.terminate?
          rescue
            # suppress termination errors - they have been logged already
          end
        end
      end

      def sync_impl(name, options)
        # Get the hostname from a tag lookup or assume it's SSH accessible directly
        hostname = get_host_by_name_or_tag(name, options, ssh_user)

        clone_commands, working_dirs = sync_available_sibling_repos(hostname, "master", "/data", ssh_user)
        update_remote_tests(hostname, "master", "/data", ssh_user)
        
        ret = 0
        if !options.skip_build?
          puts "Performing remote install..."

          out, ret = ssh(hostname, "cd /data/ ; sudo rm -rf #{working_dirs}; #{clone_commands}", 60 * 5, true, 1, ssh_user)
          print_and_exit(ret, out) if ret != 0

          out, ret = ssh(hostname, "sudo -s yum clean metadata", 60 * 2, true, 2, ssh_user) if options.clean_metadata?
          print_and_exit(ret, out) if ret != 0

          cmd = "build/devenv update"
          cmd += " --verbose" if options.verbose
          cmd += " --include_stale" if options.clean_metadata

          if(BASE_OS == "rhel" or BASE_OS == "centos")
            cmd = "scl enable ruby193 \"#{cmd}\""
          end
          out, ret = ssh(hostname, "cd /data/#{DEV_TOOLS_EXT_REPO}; sudo -s #{cmd}  2>&1 ", 60 * 15, true, 1, ssh_user) if ret == 0
          print_and_exit(ret, out) if ret != 0

          out, ret = ssh(hostname, "sudo -s #{options.clean_metadata? ? "yum update -y rhc *openshift*" : ''}", 60 * 15, true, 1, ssh_user) if options.clean_metadata?
          print_and_exit(ret, out) if ret != 0
          
          post_launch_setup(hostname)
        end

        puts "Done"
      end
  
      def build_impl(name, build_num, image, conn, options)
        $amz_options[:block_device_mappings] = {"/dev/sdb" => "ephemeral0"}

        if options.install_required_packages?
          $amz_options[:user_data] = "#!/bin/bash\nset -x\necho Defaults:root,ec2-user \!requiretty >> /etc/sudoers\nsed -i '/cat <<EOL/,$d' /etc/rc.local\nsed -i 's/PermitRootLogin without-passwordUseDNS no/PermitRootLogin without-password\nUseDNS no/g' /etc/ssh/sshd_config\n"
        else
          $amz_options[:user_data] = "#!/bin/bash\nset -x\nsed -i '/cat <<EOL/,$d' /etc/rc.local\nsed -i 's/PermitRootLogin without-passwordUseDNS no//g' /etc/ssh/sshd_config\n"
        end
        puts "Launching AMI: #{image.id} - #{image.name}"
        instance = launch_instance(image, name + "_" + build_num, 1, ssh_user)

        hostname = instance.dns_name
        puts "Done"
        puts "Hostname: #{hostname}"

=begin
        if(BASE_OS == "rhel" or BASE_OS == "centos")
          puts "TODO: remove this stuff..."
          out, ret = ssh(hostname, "sudo -s yum clean all; yum install -y ruby193-rubygem-aws-sdk", 60 * 20, true, 1, ssh_user)
          print_and_exit(ret, out) if ret != 0
          print_highlighted_output("Install Output", out)

          puts "Done"
          print_and_exit(ret, out) if ret != 0
          print_highlighted_output("Install Output", out)
        end
=end

=begin
=end

        ret, out = 0, nil
        begin
          puts "Starting yum update..."
          out, ret = ssh(hostname, "sudo -s yum -y update", 60 * 20, true, 1, ssh_user)
          print_and_exit(ret, out) if ret != 0
          print_highlighted_output("Update Output", out)

          if options.install_required_packages?
            puts "Uploading yum client certificates..."
            pem_path = File.expand_path(File.dirname(__FILE__) + "/../misc/client-cert.pem")
            scp_to(hostname, pem_path, "~/", 60*10, 5, ssh_user)
            ssh(hostname, "sudo -s mv ~/client-cert.pem /var/lib/yum/", 60 * 10, true, 1, ssh_user)
            pem_path = File.expand_path(File.dirname(__FILE__) + "/../misc/client-key.pem")
            # needed for init_repos
            scp_to(hostname, pem_path, "~/", 60*10, 5, ssh_user)
            ssh(hostname, "sudo -s mv ~/client-key.pem /var/lib/yum/", 60 * 10, true, 1, ssh_user)
            scp_to(hostname, "misc/devenv/root/.ssh/*", "~/.ssh/", 60*10, 5, ssh_user)
            ssh(hostname, "sudo -s chmod 0600 ~/.ssh/id_rsa; sudo -s chmod 0644 ~/.ssh/id_rsa.pub ~/.ssh/known_hosts;",
                           60*10, true, 1, ssh_user)
            puts "Done"

            puts "Setting up yum repos..."
            script_path = File.expand_path(File.dirname(__FILE__) + "/../misc/devenv/setup-devenv-repos.sh")
            scp_to(hostname, script_path, "~/", 60*10, 5, ssh_user)
            ssh(hostname, "sudo -s /bin/bash ~/setup-devenv-repos.sh #{options.yum_repo};", 1800, true, 1, ssh_user)
            puts "Done"

            puts "Setting up yum priorities..."
            out, ret = ssh(hostname, "sudo -s yum -y install yum-plugin-priorities", 60 * 20, true, 1, ssh_user)
            print_and_exit(ret, out) if ret != 0
            print_highlighted_output("Update Output", out)
            repo = 'rhui-REGION-rhel-server-rhscl'
            out, ret = ssh(hostname, "sudo -s yum-config-manager --setopt=#{repo}.priority=1 #{repo} --save", 60 * 20, 
                           true, 1, ssh_user)
            repo = 'rhui-REGION-rhel-server-releases'
            out, ret = ssh(hostname, "sudo -s yum-config-manager --setopt=#{repo}.priority=2 #{repo} --save", 60 * 20, 
                           true, 1, ssh_user)
            out, ret = ssh(hostname, "sudo -s yum-config-manager --setopt='#{repo}.exclude=tomcat6*' #{repo} --save", 60 * 20, 
                           true, 1, ssh_user)
            print_and_exit(ret, out) if ret != 0
            print_highlighted_output("Update Output", out)
            out, ret = ssh(hostname, "sudo -s yum install -y rh-amazon-rhui-client-jbeap6 rh-amazon-rhui-client-jbews2", 60 * 20, true, 1, ssh_user)
            print_and_exit(ret, out) if ret != 0
            print_highlighted_output("Update Output", out)
            repos = ['rhui-REGION-jbeap-6-rhui-rhel-6-rpms', 'rhui-REGION-rhel-server-6-jbews2']
            repos.each do |repo|
              out, ret = ssh(hostname, "sudo -s yum-config-manager --setopt=#{repo}.priority=3 #{repo} --save", 
                             60 * 20, true, 1, ssh_user)
              print_and_exit(ret, out) if ret != 0
              print_highlighted_output("Update Output", out)
            end

            out, ret = ssh(hostname, "sudo -s yum install -y ruby193-ruby ruby193-rubygem-thor ruby193-rubygem-capybara ruby193-rubygem-ci_reporter ruby193-rubygem-minitest ruby193-rubygem-mocha ruby193-rubygem-poltergeist ruby193-rubygem-simplecov ruby193-rubygem-webmock", 60 * 20, true, 1, ssh_user)

            print_and_exit(ret, out) if ret != 0
            print_highlighted_output("Update Output", out)
            out, ret = ssh(hostname, "sudo -s yum erase -y epel-release", 60 * 20, true, 1, ssh_user)
            print_and_exit(ret, out) if ret != 0
            print_highlighted_output("Update Output", out)
            if(BASE_OS == "rhel" or BASE_OS == "centos")
              # TODO: we shouldn't use EPEL
              out, ret = ssh(hostname, "sudo -s yum -y install http://dl.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm", 60 * 20, true, 1, ssh_user)
              print_and_exit(ret, out) if ret != 0
              print_highlighted_output("Update Output", out)
              out, ret = ssh(hostname, "sudo -s yum-config-manager --setopt=epel.enabled=0 --save epel", 60 * 10, true, 1, ssh_user)
              print_and_exit(ret, out) if ret != 0
              print_highlighted_output("Update Output", out)
            end
            
            out, ret = ssh(hostname, "sudo -s yum install -y git rubygem-thor", 60 * 10, true, 1, ssh_user)
            print_and_exit(ret, out) if ret != 0
            print_highlighted_output("Install Output", out)
          end

          puts "Creating mount..."
          if options.install_from_source? || options.install_from_local_source?
            out, ret = ssh(hostname, "sudo -s rm -rf /data", 60 * 10, true, 1, ssh_user)
            print_and_exit(ret, out) if ret != 0
          end
          #out, ret = ssh(hostname, "sudo -s umount -l /data ; if [ ! -b /dev/xvdb ]; then /sbin/mke2fs /dev/xvdb; fi; mkdir -p /data && mount /dev/xvdb /data && chown -R #{SSH_USER}:#{SSH_USER} /data/", 60 * 10, true, 1, ssh_user)
          out, ret = ssh(hostname, "sudo -s mkdir -p /data && sudo -s chown -R #{SSH_USER}:#{SSH_USER} /data/", 60 * 10, true, 1, ssh_user)
          print_and_exit(ret, out) if ret != 0

          init_repos(hostname, true, nil, "/data", ssh_user)
          clone_commands, working_dirs = '', ''

          if options.install_from_local_source?
            puts "Performing clean install from local source..."
            clone_commands, working_dirs = sync_available_sibling_repos(hostname, "master", "/data", ssh_user)
          else
            SIBLING_REPOS.each do |repo_name, repo_dirs|
              working_dirs += "#{repo_name} "
              clone_commands += "git clone #{repo_name}-bare #{repo_name}; "
              clone_commands += "pushd #{repo_name}; git checkout #{options.branch}; popd; "
            end
          end
          out, ret = ssh(hostname, "cd /data; rm -rf #{working_dirs}; #{clone_commands}", 60 * 5, true, 2, ssh_user)
          print_and_exit(ret, out) if ret != 0
          puts "Done"

          if options[:extra_rpm_dir]
            if File.exist? options[:extra_rpm_dir]
              out, ret = ssh(hostname, "mkdir -p /data/origin-server/build/extras", 60, true, 1, ssh_user)
              files = Dir.glob("#{options[:extra_rpm_dir]}/*.rpm")
              files.each do |file|
                scp_to(hostname, file, "/data/origin-server/build/extras/", 60*10, 5, ssh_user)
              end

              out, ret = ssh(hostname, "sudo -s cd /data/origin-server/build/extras && sudo -s yum install -y *.rpm", 60 * 20, true, 1, ssh_user)
            else
              puts "!!!Warning!!!"
              puts "Directory containing extra rpms not found. Skipping..."
              puts "!!!Warning!!!"
            end
          end
          
          # TODO: this doesn't need to run for install_from_source
          if(BASE_OS == "rhel" or BASE_OS == "centos")
            puts "Install build script dependencies"
            out, ret = ssh(hostname, "sudo yum install -y ruby193 scl-utils ruby193-rubygem-aws-sdk ruby193-rubygem-parseconfig", 60 * 15, true, 1, ssh_user) if ret == 0
            print_highlighted_output("Build script dependencies", out)
            out, ret = ssh(hostname, "sudo yum install -y --enablerepo=puppetlabs-products --enablerepo=epel --disablerepo=Node puppet-2.7.21 tito", 60 * 15, true, 1, ssh_user) if ret == 0
            print_highlighted_output("Build script dependencies - Puppet, Tito", out)
          end
        
          puts "Installing pre-requisite packages"
          cmd = "build/devenv install_required_packages"
          if(BASE_OS == "rhel" or BASE_OS == "centos")
            cmd = "scl enable ruby193 \"#{cmd}\""
          end
          out, ret = ssh(hostname, "cd /data/#{DEV_TOOLS_EXT_REPO}; sudo -s #{cmd} 2>&1 ", 60 * 20, true, 1, ssh_user) if ret == 0
          print_and_exit(ret, out) if ret != 0
          
          print_highlighted_output("Install Output", out)

          # Add the paths to the users .bashrc file
          #out, ret = ssh(hostname, "echo \"export PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:$PATH\" >> ~/.bashrc", 60, true, 1, ssh_user)
          selinux_level="enforcing"
          selinux_level="permissive" if options.disable_selinux?

          out, ret = ssh(hostname, %{
cat<<EOF > /tmp/selinux_config
#This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
# enforcing - SELinux security policy is enforced.
# permissive - SELinux prints warnings instead of enforcing.
# disabled - No SELinux policy is loaded.
SELINUX=#{selinux_level}
# SELINUXTYPE= can take one of these two values:
# targeted - Only targeted network daemons are protected.
# strict - Full SELinux protection.
SELINUXTYPE=targeted
EOF
          }, 60 * 2, true, 1, ssh_user)
          print_and_exit(ret, out) if ret != 0
          out, ret = ssh(hostname, "sudo -s cp /tmp/selinux_config /etc/selinux/config", 60 * 15, true, 1, ssh_user) if ret == 0
          print_and_exit(ret, out) if ret != 0
          if options.disable_selinux?
            out, ret = ssh(hostname, "sudo -s /usr/sbin/setenforce 0", 60 * 15, true, 1, ssh_user) if ret == 0
            print_and_exit(ret, out) if ret != 0
          end

          if options.install_required_packages?
            puts "Setting up puppet modules..."
            base_os = guess_os(options.base_os)

            #puppet version 2.7
            out, ret = ssh(hostname, %{sudo -s rm -rf /etc/puppet/modules &&
              sudo -s mkdir -p /etc/puppet/modules &&
              sudo -s puppet module install puppetlabs/stdlib &&
              sudo -s puppet module install puppetlabs/ntp &&
              sudo -s ln -sf /data/puppet-openshift_enterprise /etc/puppet/modules/openshift_origin
              }, 60 * 20, true, 1, ssh_user)
            print_and_exit(ret, out) if ret != 0
          else
            if options.install_from_source? || options.install_from_local_source?
              puts "Installing Origin packages..."
              cmd = "build/devenv local_build --clean-packages"
              if(BASE_OS == "rhel" or BASE_OS == "centos")
                cmd = "scl enable ruby193 \"#{cmd}\""
              end
              out, ret = ssh(hostname, "cd /data/#{DEV_TOOLS_EXT_REPO}; sudo -s #{cmd}  2>&1 ", 60 * 30, true, 1, ssh_user) if ret == 0
              print_and_exit(ret, out) if ret != 0
            
              out = ssh(hostname, "sudo -s rpm -qa | grep openshift", 60, false, 1, ssh_user)
              print_highlighted_output("Installed Origin Packages", out)
            
            end
          end
          puts "Done"          

          # Cleanup any lines that break sshd_config (cloud-init RHEL 6.4 bug)
          puts "Cleaning up broken sshd_config (RHEL 6.4 cloud-init bug)"
          ssh(hostname, 'sudo -s sed -i "s/PermitRootLogin without-passwordUseDNS no//g" /etc/ssh/sshd_config', 60, true, 1, ssh_user)

          image_id = nil
          if options[:register]
            # reset the eth0 network config to remove the HWADDR
            puts "Removing HWADDR and DNS entries from eth0 network config..."
            reset_eth0_dns_config(hostname)
            
            manifest = rpm_manifest(hostname, ssh_user)              
            registered_ami = register_image(conn, instance, name + '_' + build_num, manifest)
            image_id = registered_ami.id
          end

          unless options.install_required_packages?  
            puts "Running post_launch_setup..."
            post_launch_setup(hostname)
          end

          unless options.skip_verify? || options.install_required_packages?
            update_remote_tests(hostname, options.branch, "/data", ssh_user)
            test_impl(name + '_' + build_num, hostname, instance, conn, options, image_id)
          end
          puts "Done."
        ensure
          begin
            terminate(name + '_' + build_num) if options.terminate?
          rescue
            # suppress termination errors - they have been logged already
          end
        end
      end

      def reset_eth0_dns_config(hostname)
cmd = %{
sudo -s echo \\\"DEVICE=eth0
BOOTPROTO=dhcp
ONBOOT=yes
\\\" > /etc/sysconfig/network-scripts/ifcfg-eth0

sudo -s /etc/init.d/network restart
sudo -s service named restart
}
        out, ret = ssh(hostname, cmd, 60 * 5, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0
      end
  
      def sanity_check_impl(tag, hostname, instance, conn, options, image_id=nil)
        threads = []
        failures = []
        titles = ["OpenShift Origin Broker Sanity",
                  "OpenShift Origin Node Unit"]
    
        cmds = ["cd /var/www/openshift/broker; sudo -s bundle exec rake test:sanity",
                "cd /data/openshift-test/node; sudo -s rake unit_test"]
        add_ssh_cmd_to_threads(hostname, threads, failures, titles, cmds, false, @@SSH_TIMEOUT, ssh_user)
        add_ssh_cmd_to_threads(hostname, threads, failures, "Cucumber Sanity", "cucumber #{CUCUMBER_OPTIONS} -t @sanity /data/openshift-test/tests/", false, @@SSH_TIMEOUT, ssh_user)
    
        threads.each do |t|
          t[0].join
        end
    
        unless failures.empty?
          failures.uniq!
          retry_test_failures(hostname, failures, 1, @@SSH_TIMEOUT, ssh_user)
        end
      end
      
      def update_facts_impl(hostname)
      end
      
      def post_launch_setup(hostname)                        
        puts "Setting machine hostname"
        out, ret = ssh(hostname, %{
cat<<EOF > ~#{SSH_USER}/configure_hostname.pp
file { "update network settings - hostname": 
  path    => "/etc/sysconfig/network",
  content => "NETWORKING=yes\nNETWORKING_IPV6=no\nHOSTNAME=\\\${ec2_public_hostname}\n"
}
exec { "set hostname":
  command => "/bin/hostname \\\${ec2_public_hostname}"
}
EOF
        }, 60 * 2, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0
        out, ret = ssh(hostname, %{
cat<<EOF > ~#{SSH_USER}/configure_origin.pp
\\\$keyfile="/var/named/Kexample.com.*.key"
\\\$key=inline_template("<%=File.read(Dir.glob(keyfile)[0]).strip.split(\' \')[7]%>")
class { "openshift_origin" :
  node_fqdn              => \\\$ec2_public_hostname,
  cloud_domain           => "example.com",
  named_tsig_priv_key    => \\\$key,
  dns_servers            => ["172.16.0.23"],
  os_unmanaged_users     => ["#{SSH_USER}"],
  install_repo           => "file:///data/origin-rpms",
  development_mode       => true,
  configure_cgroups      => true,
  use_v2_carts           => true,
  broker_auth_plugin     => "htpasswd",
  configure_ntp          => false,
  broker_auth_salt       => 'ClWqe5zKtEW4CJEMyjzQ2',
  broker_session_secret  => 'devenv_secret',
  console_session_secret => 'devenv_secret',
  mongo_auth_password    => 'devenv_mongo_password'
}
EOF
        }, 60 * 2, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0

        # reset the eth0 network config to add the dns entries
        out, ret = ssh(hostname, "sudo -s puppet apply --verbose ~#{SSH_USER}/configure_hostname.pp", 60 * 20, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0
        
        ssh(hostname, 'sudo -s rm -rf /var/named/Kexample.com*', 60 * 20, true, 1, ssh_user)
        out, ret = ssh(hostname, 'sudo -s yum install -y bind', 60 * 20, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0
        out, ret = ssh(hostname, 'sudo -s /usr/sbin/dnssec-keygen -a HMAC-MD5 -b 512 -n USER -r /dev/urandom -K /var/named example.com', 60 * 20, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0
        out, ret = ssh(hostname, 'TSIG_KEY=`sudo -s cat /var/named/Kexample.com.*.key | awk "{ print $8 }"` ; echo $TSIG_KEY', 60 * 20, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0

        out, ret = ssh(hostname, "sudo -s puppet apply --verbose ~#{SSH_USER}/configure_origin.pp", 60 * 30, true, 1, ssh_user)
        print_and_exit(ret,out) if ret != 0
        print_highlighted_output( "Puppet", out )

        # TODO: activemq is failing to start or crashing
        out, ret = ssh(hostname, "sudo -s service activemq restart; sleep 10; sudo -s service mcollective restart", 60 * 20, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0
        print_highlighted_output( "Service Restart", out)
        restart_services_remote(hostname)

        # TODO: find out why the %post section of the cartridge rpms is failing without showing up in output
        out, ret = ssh(hostname, "sudo -s oo-admin-cartridge -a install -R -s /usr/libexec/openshift/cartridges/v2/", 60 * 20, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0
        print_highlighted_output( "Verify all cartridges installed into cartridge repository", out)
       
        # Remove abrt-addon-python until https://bugzilla.redhat.com/show_bug.cgi?id=907449 is resolved
        out, ret = ssh(hostname, "sudo -s yum remove -y abrt-addon-python", 60 * 20, true, 1, ssh_user)
        print_highlighted_output( "Removing abrt-addon-python for bz907449", out)
      end

      def restart_services
        cmd = %{
        # Cleanup any lines that break sshd_config (cloud-init RHEL 6.4 bug)
        sudo -s sed -i "s/PermitRootLogin without-passwordUseDNS no//g" /etc/ssh/sshd_config
        sudo -s service activemq restart; sudo -s service mongod restart
        sudo -s service cgconfig restart; sudo -s service cgred restart ; sudo -s service openshift-cgroups restart
        sudo -s service httpd restart; sudo -s service openshift-broker restart
        sudo -s service openshift-console restart; 
        sudo -s service openshift-node-web-proxy stop; sudo -s service openshift-port-proxy stop
        sudo -s service named restart; sudo -s service network restart
        sudo -s service mcollective restart; sudo -s service oddjobd restart
        sudo -s service openshift-node-web-proxy start; sudo -s service openshift-port-proxy start
        sudo -s service sshd restart
        }
        run(cmd)
      end

      def restart_services_remote(hostname)
        puts "Restarting services..."

        cmd = %{
# Cleanup any lines that break sshd_config (cloud-init RHEL 6.4 bug)
sudo -s sed -i "s/PermitRootLogin without-passwordUseDNS no//g" /etc/ssh/sshd_config
sudo -s service activemq restart; sudo -s service mongod restart
sudo -s service httpd restart; sudo -s service openshift-broker restart
sudo -s service openshift-console restart; 
sudo -s service openshift-node-web-proxy stop; sudo -s service openshift-port-proxy stop
sudo -s service named restart; sudo -s service network restart
sudo -s service cgconfig restart; sudo -s service cgred restart ; sudo -s service openshift-cgroups restart
sudo -s service mcollective restart; sudo -s service oddjobd restart
sudo -s service openshift-node-web-proxy start; sudo -s service openshift-port-proxy start
sudo -s service sshd restart
}
        out, ret = ssh(hostname, cmd, 60 * 5, true, 1, ssh_user)
        print_and_exit(ret, out) if ret != 0
        puts "Done"
      end
    end # no_tasks end
  end # class end
end # module end
Origin::BuilderPlugin.start
